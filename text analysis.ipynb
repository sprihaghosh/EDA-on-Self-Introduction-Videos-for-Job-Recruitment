{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9ab1121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b1b0460",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"C:/Users/HP/Desktop/I'mBesideYou/transcripts/\"\n",
    "output_dir = \"C:/Users/HP/Desktop/I'mBesideYou/processed_text/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0643f65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d02ad963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_file(input_path, output_path):\n",
    "    with open(input_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Stop word removal and lowercasing\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word.lower() for word in tokens if word.isalnum() and word.lower() not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    \n",
    "    # Save the processed text\n",
    "    with open(output_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(\" \".join(lemmatized_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7a6c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    input_path = os.path.join(input_dir, f\"{i}.txt\")\n",
    "    output_path = os.path.join(output_dir, f\"{i}_processed.txt\")\n",
    "    \n",
    "    process_text_file(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1931bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define the input and output directories\n",
    "input_dir = \"C:/Users/HP/Desktop/I'mBesideYou/processed_text/\"\n",
    "output_dir = \"C:/Users/HP/Desktop/I'mBesideYou/processed_text/\"\n",
    "\n",
    "# Function to perform NER and replace entities in a single text file\n",
    "def perform_ner_and_replace(input_path, output_path):\n",
    "    with open(input_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Process text with spaCy for NER\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Replace entities in the text with their labels\n",
    "    replaced_text = \" \".join([ent.text if ent.ent_type_ else ent.text for ent in doc])\n",
    "    \n",
    "    # Save the text with replaced entities\n",
    "    with open(output_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(replaced_text)\n",
    "\n",
    "# Process all 10 candidate text files and replace entities\n",
    "for i in range(1, 11):\n",
    "    input_path = os.path.join(input_dir, f\"{i}_processed.txt\")\n",
    "    output_path = os.path.join(output_dir, f\"{i}_processed.txt\")\n",
    "    \n",
    "    perform_ner_and_replace(input_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e27210b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK data (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define a function to preprocess and clean the text\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lowercasing\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Stop word removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join the tokens back into a cleaned text\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Loop through the 10 CSV files\n",
    "for i in range(1, 11):\n",
    "    # Load the CSV file\n",
    "    file_path = f'C:\\\\Users\\\\HP\\\\Desktop\\\\I\\'mBesideYou\\\\overall_data\\\\overall_data_{i}.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Apply preprocessing to the \"text\" column and create a new \"cleaned_text\" column\n",
    "    df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "    \n",
    "    # Save the cleaned data back to the same CSV file\n",
    "    df.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae888d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK data (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define a function to preprocess and clean the text\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lowercasing\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Stop word removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join the tokens back into a cleaned text\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Keywords for each category\n",
    "education_keywords = [\"postgraduate\", \"management\", \"iim\", \"degree\", \"bba\", \"mba\", \"varanasi university\",\n",
    "                     \"engineering\", \"graduate\", \"data science\", \"python\", \"data analysis\", \"mass media\",\n",
    "                     \"entrepreneurship\", \"analytics\", \"commerce\"]\n",
    "\n",
    "skills_keywords = [\"attention\", \"analyze\", \"detail\", \"consistency\", \"perseverance\", \"flexible attitude\",\n",
    "                   \"passionate\", \"passion\", \"consulting\", \"adaptive\", \"inquisitive\", \"learner\",\n",
    "                   \"public relation\", \"strategy\", \"planning\", \"writer\", \"editor\", \"analytical skill\",\n",
    "                   \"finance\", \"analytical\", \"entrepreneurship\", \"skill\"]\n",
    "\n",
    "experience_keywords = [\"regulatory affair\", \"risk management\", \"three years\", \"investment\", \"framework venture\",\n",
    "                      \"startup business\", \"business model\", \"valuation\", \"finance model\", \"welfare society\",\n",
    "                      \"sale associate\", \"advisor\", \"administration\", \"two year\", \"consulting experience\",\n",
    "                      \"validation process software\", \"insurance\", \"underwriting\", \"statutory audit\", \"ca\",\n",
    "                      \"audit\", \"startup\", \"leading project\", \"business development\", \"accounting associate\",\n",
    "                      \"tax associate\"]\n",
    "\n",
    "reasons_to_join_keywords = [\"looking challenging role\", \"skill practice\", \"believe\", \"fit\", \"grow\", \"acceleration\",\n",
    "                            \"rewarding experience\", \"company flourish\", \"aspiring result\", \"share idea\",\n",
    "                            \"program strategy\", \"spread awareness\", \"positive attitude\", \"affinity\", \"strategizing\",\n",
    "                            \"consultant\", \"love learning\", \"want join\", \"newer experience\", \"create society\",\n",
    "                            \"child live\", \"uniqueness\", \"resonated\", \"part\", \"awakening\", \"edtech space\", \"believe\",\n",
    "                            \"fascinated\", \"expand\", \"successful model\", \"aim\", \"noble cause\", \"challenging\",\n",
    "                            \"development\", \"superior\", \"best outcome\"]\n",
    "\n",
    "# Function to check if any keyword from the provided list is present in the text\n",
    "def check_keywords(text, keyword_list):\n",
    "    for keyword in keyword_list:\n",
    "        if keyword in text:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# Loop through the 10 CSV files\n",
    "for i in range(1, 11):\n",
    "    # Load the CSV file\n",
    "    file_path = f'C:\\\\Users\\\\HP\\\\Desktop\\\\I\\'mBesideYou\\\\overall_data\\\\overall_data_{i}.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Apply preprocessing to the \"text\" column and create new columns\n",
    "    df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "    \n",
    "    # Check for presence of keywords and create new columns\n",
    "    df['education'] = df['cleaned_text'].apply(lambda x: check_keywords(x, education_keywords))\n",
    "    df['skills'] = df['cleaned_text'].apply(lambda x: check_keywords(x, skills_keywords))\n",
    "    df['experience'] = df['cleaned_text'].apply(lambda x: check_keywords(x, experience_keywords))\n",
    "    df['reasons_to_join'] = df['cleaned_text'].apply(lambda x: check_keywords(x, reasons_to_join_keywords))\n",
    "    \n",
    "    # Save the cleaned and enriched data back to the same CSV file\n",
    "    df.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7118f2ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
